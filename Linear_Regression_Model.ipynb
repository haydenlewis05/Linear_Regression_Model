{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77886ae3",
   "metadata": {},
   "source": [
    "# Computation Physics & Data Science: Exercise 3 - Large Dataset Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dea559",
   "metadata": {},
   "source": [
    "## Libary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbe612e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression, QuantileRegressor, HuberRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_validate, cross_val_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e829cc",
   "metadata": {},
   "source": [
    "## Part 1: Reading and Verifying Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537c32ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#==============================\n",
    "#IMPORT RAW DATA & CLEAN HEADER\n",
    "#==============================\n",
    "rows_skipped = [0, 2, 3] #Decorative/ unit lines in data file to skip\n",
    "\n",
    "df = pd.read_table(\"data.csv\", skiprows=rows_skipped, delimiter=\",\",on_bad_lines=\"skip\") #Creates panda data frame with the csv file\n",
    "\n",
    "\n",
    "#=============\n",
    "#DATA CLEANING\n",
    "#=============\n",
    "df.columns = df.columns.str.replace('#', '').str.strip() #Remove special characters from header\n",
    "\n",
    "for col in df.columns[1:]: #Loop through columns skipping first materials column \n",
    "    df[col] = pd.to_numeric(df[col], errors=\"coerce\") #Convert to int & turn errors into NA values   \n",
    "\n",
    "df = df.drop(df[df['material'] == 'm'].index) #Removes invalid 'm' material row\n",
    "df = df.dropna() #Drop all other NA cells\n",
    "\n",
    "\n",
    "#==============\n",
    "#MIN & MAX DATA\n",
    "#==============\n",
    "#Store array of min and maximum of each feature\n",
    "feature_max = df.max()\n",
    "feature_min = df.min()\n",
    "\n",
    "#print out each features min and max values\n",
    "feature_names = ['density', 'radius', 'mass', 'temperature', 'pressure', 'height', 'time']\n",
    "for name in feature_names:\n",
    "    print(f\"{name}: Maximum = {feature_max[name]:.4f}, Minimum = {feature_min[name]:.4f}\")\n",
    "\n",
    "\n",
    "#===========\n",
    "#OUTPUT DATA\n",
    "#===========\n",
    "df #Print first and last few rows of data (just to visulise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513175a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=====\n",
    "#PLOTS\n",
    "#=====\n",
    "#Height-Time per Material\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=df, x='height', y='time', hue='material') #Utalises seaborn libary w/ panda tables\n",
    "plt.title('Fall Height vs Time by Material')\n",
    "plt.xlabel(\"Fall Height (m)\")\n",
    "plt.ylabel(\"Time (s)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "#Height-Time per Sphere Radius\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=df, x='height', y='time', hue='radius')\n",
    "plt.title('Fall Height vs Time by Sphere Radius')\n",
    "plt.xlabel(\"Fall Height (m)\")\n",
    "plt.ylabel(\"Time (s)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fd93e8",
   "metadata": {},
   "source": [
    "## Part 2: Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cae545",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================\n",
    "#HEATMAP W/ SEABORN\n",
    "#==================\n",
    "plt.figure(figsize=(10, 6))\n",
    "heatmap = sns.heatmap(df.corr(), cmap='coolwarm', vmin=-1, vmax=1, annot=True) #Inbuilt heatmap and correlation functions\n",
    "heatmap.set_title('Correlation Heatmap')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7426b8",
   "metadata": {},
   "source": [
    "## Part 3: Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344f3e12",
   "metadata": {},
   "source": [
    "### 3a: MSE Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dca087",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X = df[['density', 'radius', 'mass', 'temperature', 'pressure', 'height']].values #Define matrix of variable data\n",
    "y = df['time'].values #Defines our actual observed results\n",
    "\n",
    "\n",
    "#=================\n",
    "#MSE LOSS FUNCTION\n",
    "#=================\n",
    "model_mse = LinearRegression() #Model is set to linear regression (used for MSE)\n",
    "model_mse.fit(X, y) #Trains the model with our X matrix and its target y values\n",
    "\n",
    "betas_mse = model_mse.coef_ #Creates database of model coefficients\n",
    "intercept_mse = model_mse.intercept_ #Saves intercept of model\n",
    "\n",
    "yp_mse = model_mse.predict(X) #Creates predicted y (time) values\n",
    "\n",
    "mse = mean_squared_error(y, yp_mse) #Computes mse equation w/ true and predicted y (time) values to determine loss\n",
    "\n",
    "print(f'Mean Squared Error (MSE) Loss: {mse:.6f}') #Output mse loss value\n",
    "\n",
    "#Output best fit line coefficients and intercept\n",
    "print(f'Intercept: {intercept_mse:.6f}')\n",
    "for name, beta in zip(feature_names, betas_mse):\n",
    "    print(f\"{name}: {beta:.6f}\")\n",
    "    \n",
    "\n",
    "#==========================\n",
    "#VISULISING DATA (2D PLOTS)\n",
    "#==========================\n",
    "#HEIGHT MODEL (using same methods as before but trained only on height data)\n",
    "model_height = LinearRegression()\n",
    "model_height.fit(df[['height']], y) \n",
    "\n",
    "betas_height = model_height.coef_\n",
    "intercept_height = model_height.intercept_\n",
    "\n",
    "yp_height = model_height.predict(df[['height']])  \n",
    "\n",
    "mse_height = mean_squared_error(y, yp_height)\n",
    "\n",
    "print(f'\\nMean Squared Error (MSE) Height Loss: {mse_height:.6f}')  \n",
    "\n",
    "#Output line of best fit beta weights\n",
    "print(f'Intercept: {intercept_height:.6f}')  \n",
    "print(f\"Height Beta: {betas_height[0]:.6f}\") \n",
    "\n",
    "#Plot height data in scatter\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=df, x='height', y='time', color='blue')\n",
    "\n",
    "#Plot height-time regression line\n",
    "plt.plot(df['height'], yp_height, color='red', linewidth=2, label='Regression Line (MSE)')\n",
    "plt.xlabel('Height (m)')\n",
    "plt.ylabel('Time (s)')\n",
    "plt.title('Linear Regression (MSE): Height vs Time')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#RADIUS MODEL\n",
    "model_radius = LinearRegression()\n",
    "model_radius.fit(df[['radius']], y) \n",
    "\n",
    "betas_radius = model_radius.coef_\n",
    "intercept_radius = model_radius.intercept_\n",
    "\n",
    "yp_radius = model_radius.predict(df[['radius']])  \n",
    "\n",
    "mse_radius = mean_squared_error(y, yp_radius)\n",
    "\n",
    "print(f'Mean Squared Error (MSE) Radius Loss: {mse_radius:.6f}')  \n",
    "\n",
    "#Output line of best fit beta weights\n",
    "print(f'Intercept: {intercept_radius:.6f}')  \n",
    "print(f\"Radius Beta: {betas_radius[0]:.6f}\") \n",
    "\n",
    "#Plot radius data in scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=df, x='radius', y='time', color='blue')\n",
    "\n",
    "#Plot radius-time regression line\n",
    "plt.plot(df['radius'], yp_radius, color='red', linewidth=2, label='Regression Line (MSE)')\n",
    "plt.xlabel('Radius (m)')\n",
    "plt.ylabel('Time (s)')\n",
    "plt.title('Linear Regression (MSE): Radius vs Time')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#DENSITY MODEL\n",
    "model_density = LinearRegression()\n",
    "model_density.fit(df[['density']], y) \n",
    "\n",
    "betas_density = model_density.coef_\n",
    "intercept_density = model_density.intercept_\n",
    "\n",
    "yp_density = model_density.predict(df[['density']])  \n",
    "\n",
    "mse_density = mean_squared_error(y, yp_density)\n",
    "\n",
    "print(f'Mean Squared Error (MSE) Density Loss: {mse_density:.6f}')  \n",
    "\n",
    "#Output line of best fit beta weights\n",
    "print(f'Intercept: {intercept_density:.6f}')  \n",
    "print(f\"Density Beta: {betas_density[0]:.6f}\") \n",
    "\n",
    "#Density scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=df, x='density', y='time', color='blue')\n",
    "\n",
    "#Plot density-time regression line\n",
    "plt.plot(df['density'], yp_density, color='red', linewidth=2, label='Regression Line (MSE)')\n",
    "plt.xlabel('Density (kg/m^3)')\n",
    "plt.ylabel('Time (s)')\n",
    "plt.title('Linear Regression (MSE): Density vs Time')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44307b87",
   "metadata": {},
   "source": [
    "### 3b: Additional Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa332664",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===================\n",
    "#MEAN ABSOLUTE ERROR\n",
    "#===================\n",
    "model_mae = QuantileRegressor(quantile=0.5, solver='highs') #Fits through median target values where minimum MAE loss lies\n",
    "model_mae.fit(X, y) #Trains our model to pur dataset\n",
    "betas_mae = model_mae.coef_ #Defines beta values\n",
    "intercept_mae = model_mae.intercept_ \n",
    "yp_mae = model_mae.predict(X) #Predicts y values for our model\n",
    "mae = mean_absolute_error(y, yp_mae) #Computes loss from our actual and expected data\n",
    "\n",
    "#Output MAE model results\n",
    "print(f'Mean Absolute Error (MAE) Loss: {mae:.6f}')\n",
    "print(f'Intercept: {intercept_mae:.6f}')\n",
    "for name, beta in zip(feature_names, betas_mae):\n",
    "    print(f\"{name}: {beta:.6f}\")\n",
    "    \n",
    "#=====\n",
    "#HUBER\n",
    "#=====\n",
    "model_huber = HuberRegressor() #Initalise huber regression\n",
    "model_huber.fit(X, y) #Train \n",
    "betas_huber = model_huber.coef_ #Gather betas\n",
    "intercept_huber = model_huber.intercept_ \n",
    "yp_huber = model_huber.predict(X) #Predict target values\n",
    "\n",
    "#Huber loss function using mathematical function as shown in report\n",
    "def huber_loss(delta, residuals):\n",
    "    #If else array array to return the loss array\n",
    "    return np.where(np.abs(residuals) <= delta, 0.5 * residuals**2, delta * (np.abs(residuals) - 0.5 * delta))\n",
    "\n",
    "#Calculate Huber model loss\n",
    "delta = 1.0 #Condition for MSE and MAE transition\n",
    "huber = np.mean(huber_loss(delta, y - yp_huber))\n",
    "\n",
    "#Output Huber results\n",
    "print(f\"\\nHuber Loss: {huber:.6f}\")\n",
    "print(f\"Intercept: {intercept_huber:.6f}\")\n",
    "for name, beta in zip(feature_names, betas_huber):\n",
    "    print(f\"{name}: {beta:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e8150f",
   "metadata": {},
   "source": [
    "### 3c: Gradient Decent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3b6548",
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 1e-3 #Global learning-rate \n",
    "n_iterations = 1000 #Number of iterations for each descent\n",
    "\n",
    "scaler = StandardScaler() #Creates scalar object to store mean and std of each variable\n",
    "X_normalised = scaler.fit_transform(X) #Normalises each variable to avoid overflow errors w/ learning rates \n",
    "\n",
    "N = X_normalised.shape[0] #Number of datapoints\n",
    "X_normalised = np.c_[np.ones((N, 1)), X_normalised] #Joins together objects w/ extra column for intercept term descent\n",
    "\n",
    "print(f\"Learning Rate: {eta}\")\n",
    "print(f\"No. Iterations: {n_iterations}\")\n",
    "\n",
    "#Function to allow for epeated use of standardising our beta weights\n",
    "def unnormalise(betas):\n",
    "    for i in range(1, len(betas)):\n",
    "        betas[i] /= scaler.scale_[i-1] #Divide by std \n",
    "        betas[0] -= betas[i] * scaler.mean_[i-1] #Correct intercept\n",
    "    \n",
    "    return betas\n",
    "\n",
    "\n",
    "#======================\n",
    "#BATCH GRADIENT DESCENT\n",
    "#======================\n",
    "#Uses entire datafile to calculate gradient in each iteration\n",
    "\n",
    "betas_bgd = np.zeros((7, 1)) #Creates 'empty' list of zeros for beta values\n",
    "betas_bgd_path = [] #Creates empty array to store each iteration of the beta values\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = (2/N) * X_normalised.T.dot(X_normalised.dot(betas_bgd) - yp_mse.reshape(-1,1)) #Calculates gradients using whole datafile\n",
    "    \n",
    "    betas_bgd = betas_bgd - eta * gradients #Creates new value for beta in step size of learning rate in direction of minima (direction of gradient)\n",
    "    betas_bgd_path.append(betas_bgd.copy()) #Saves copy of new beta values\n",
    "\n",
    "betas_bgd_standard = unnormalise(betas_bgd) #Unnormalise the beta weights\n",
    "\n",
    "#Calculate MSE loss\n",
    "X_original = np.c_[np.ones((N, 1)), X] #Creates object of w/ X variables as well as intercept term\n",
    "mse_bgd = mean_squared_error(X_original.dot(betas_bgd_standard), yp_mse) #Used to calculate MSE loss\n",
    "\n",
    "#Print results\n",
    "print(f'\\nBatch Gradient Descent Results: {mse_bgd:.6f}')\n",
    "for name, beta in zip([\"Intercept\"] + feature_names, betas_bgd_standard):\n",
    "    print(f\"{name}: {beta[0]:.6f}\")\n",
    "\n",
    "    \n",
    "#===========================\n",
    "#STOCHASTIC GRADIENT DESCENT\n",
    "#===========================\n",
    "#Uses a single random element each iteration to calculate gradient\n",
    "\n",
    "betas_sgd = np.zeros((7,1))\n",
    "betas_sgd_path = []\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    random_index = np.random.randint(N) #Calculates random number (index) wihtin datafile size N\n",
    "    x1 = X_normalised[random_index:random_index+1] #Finds random row\n",
    "    y1 = yp_mse[random_index:random_index+1] #Finds random element in that row\n",
    "    \n",
    "    gradients = 2 * x1.T.dot(x1.dot(betas_sgd) - y1) #Gradient calculated for that element\n",
    "    \n",
    "    betas_sgd = betas_sgd - eta * gradients \n",
    "    betas_sgd_path.append(beta)\n",
    "\n",
    "betas_sgd_standard = unnormalise(betas_sgd.copy()) #Unnormalise weights\n",
    "\n",
    "#Calculate MSE loss\n",
    "X_original = np.c_[np.ones((N, 1)), X]\n",
    "mse_sgd = mean_squared_error(X_original.dot(betas_sgd_standard), yp_mse)\n",
    "\n",
    "#Output final beta weights\n",
    "print(f'\\nStochastic Gradient Descent Results: {mse_sgd:.6f}')\n",
    "for name, beta in zip([\"Intercept\"] + feature_names, betas_sgd_standard):\n",
    "    print(f\"{name}: {beta[0]:.6f}\")\n",
    "\n",
    "    \n",
    "#===========================    \n",
    "#MINI-BATCH GRADIENT DESCENT\n",
    "#===========================\n",
    "#Uses small random subset of data to calculate gradient at each iteration\n",
    "\n",
    "betas_mgd = np.zeros((7, 1))\n",
    "betas_mgd_path = []\n",
    "\n",
    "minibatch_size = 10 #Size of each mini-batch sample\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    #Shuffle data\n",
    "    shuffled_indices = np.random.permutation(N) #Shuffles matrix index\n",
    "    X_shuffled = X_normalised[shuffled_indices] #Shuffles columns according to shuffled index\n",
    "    y_shuffled = yp_mse.reshape(-1,1)[shuffled_indices] #Shuffles values according to shuffled columns\n",
    "    \n",
    "    #Select subset of shuffled data\n",
    "    xi = X_shuffled[:minibatch_size] #Selects first initial mini-batch of features\n",
    "    yi = y_shuffled[:minibatch_size] #Selects corresponding values\n",
    "    \n",
    "    gradients = 2/minibatch_size * xi.T.dot(xi.dot(betas_mgd) - yi) #Gradient for mini-batch group\n",
    "    \n",
    "    #Update beta values accordingly\n",
    "    betas_mgd = betas_mgd - eta * gradients\n",
    "    betas_mgd_path.append(betas_mgd.copy())\n",
    "\n",
    "betas_mgd_standard = unnormalise(betas_mgd) #Unnormalise beta weights\n",
    "\n",
    "#Calculate MSE loss\n",
    "X_original = np.c_[np.ones((N, 1)), X]\n",
    "mse_mgd = mean_squared_error(X_original.dot(betas_mgd_standard), yp_mse)\n",
    "\n",
    "#Output weights & intercept\n",
    "print(f'\\nMini-batch Gradient Descent Results: {mse_mgd:.6f}')\n",
    "for name, beta in zip([\"Intercept\"] + feature_names, betas_mgd_standard):\n",
    "    print(f\"{name}: {beta[0]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c04c681",
   "metadata": {},
   "source": [
    "## Part 4: Model Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e64f3c9",
   "metadata": {},
   "source": [
    "### 4a: Train & Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3a9001",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#============\n",
    "#TRAIN & TEST\n",
    "#============\n",
    "#Creating train & test data subsets to understand models reaction to unseen data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "model_mse.fit(X_train, y_train) #Plot train dataset to mse model (90% of datafile)\n",
    "\n",
    "betas_test = np.zeros((7, 1)) #Create empty array for test betas\n",
    "betas_test = model_mse.coef_ #Fit to linear model as trained above with train dataset\n",
    "intercept_test = model_mse.intercept_\n",
    "\n",
    "#Output weights & intercept for test dataset\n",
    "print(f'\\nTest Betas Results:')\n",
    "print(f'Intercept: {intercept_test}')\n",
    "for name, beta in zip(feature_names, betas_test):\n",
    "    print(f'{name}: {beta:.6f}')\n",
    "\n",
    "residuals = X_test.dot(betas_test) - y_test + intercept_test #Calculate residuals of train and test datasets w/ intercept to normalise\n",
    "\n",
    "#Residual mean and standard deviation\n",
    "residual_mean = np.mean(residuals)\n",
    "residual_std = np.std(residuals)\n",
    "print(f\"\\nResiduals (Mean & Std): {residual_mean:.6f} ± {residual_std:.6f}\")\n",
    "\n",
    "#Validate the model with train and test scores\n",
    "model_validate = linear_model.LinearRegression()\n",
    "valid_scores = cross_validate(model_validate, X, y, return_train_score = True) #Cross validadtion returns model performance to train and test datasets\n",
    "#valid_scores['train_score']\n",
    "\n",
    "#Scores how well model fits training data\n",
    "print(f\"\\nTrain Score (Mean & Std): {np.mean(valid_scores['train_score']):.4f} ± {np.std(valid_scores['train_score']):.6f}\")\n",
    "#Scores how well models to unseen data\n",
    "print(f\"Test Score (Mean & Std): {np.mean(valid_scores['test_score']):.4f} ± {np.std(valid_scores['test_score']):.6f}\")\n",
    "\n",
    "\n",
    "#=====\n",
    "#PLOTS\n",
    "#=====\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(range(len(residuals)), residuals, color = 'b', alpha=0.75)\n",
    "plt.axhline(y = residual_mean + residual_std, color='r', linewidth=2, label = 'Standard Deviation')\n",
    "plt.axhline(y = residual_mean - residual_std, color='r', linewidth=2)\n",
    "plt.axhline(y = residual_mean, color='g',linewidth=2, label = 'Residual Mean')\n",
    "plt.xlabel('Dataset Index')\n",
    "plt.ylabel('Residual')\n",
    "plt.title('Train vs Test Data Residuals')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(residuals, bins=30, color='b', alpha=0.75, edgecolor='black', label='Residuals')\n",
    "plt.axvline(x=residual_mean, color='g', linewidth=2, label='Residual Mean')\n",
    "plt.axvline(x=residual_mean + residual_std, color='r', linewidth=2, label='Standard Deviation')\n",
    "plt.axvline(x=residual_mean - residual_std, color='r', linewidth=2)\n",
    "plt.xlabel('Residual Value')\n",
    "plt.ylabel('No. Instances')\n",
    "plt.title('Train vs Test Data Residuals Histogram')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378031b8",
   "metadata": {},
   "source": [
    "### 4b: Physical Data Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66923f3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#===================\n",
    "#DETERMINE CONSTANTS\n",
    "#===================\n",
    "M = 0.0289652 #kg/mol, molar mass of dry air\n",
    "R = 8.314462 #J/(mol*K), molar gas constant\n",
    "C_d = 0.47 #Assumed drag coefficient of a sphere\n",
    "g = 9.81 #m/s^2, acceleration due to gravity \n",
    "\n",
    "\n",
    "#===============\n",
    "#QUADRATIC MODEL\n",
    "#===============\n",
    "def fall_time(h, m, r, p, T):\n",
    "    #Calculate air density\n",
    "    rho = (p * M) / (R * T)\n",
    "    \n",
    "    #Calculate cross-sectional areas of sphere\n",
    "    A = np.pi * r**2\n",
    "    \n",
    "    #Calculate drag coefficient\n",
    "    k = (C_d * rho * A) / 2\n",
    "    \n",
    "    #Calculate fall time\n",
    "    time = np.sqrt(m / (k * g)) * np.arccosh(np.exp((h * k) / m))\n",
    "\n",
    "    return time\n",
    "\n",
    "#Create height arrays\n",
    "h_low = np.linspace(0,500,1000)\n",
    "h_high = np.linspace(1000,1500,1000)\n",
    "\n",
    "#Find sample data to be used in above and below models \n",
    "#In future model w/ more time two values for below and above height range would be found to better simulate air resistance at those heights\n",
    "sample = df.iloc[0]\n",
    "m = sample['mass']\n",
    "r = sample['radius']\n",
    "p = sample['pressure']\n",
    "T = sample['temperature']\n",
    "\n",
    "#Calculate fall time above and below our data w/ quadratic model for each height interval\n",
    "t_low = [fall_time(h, m, r, p, T) for h in h_low]\n",
    "t_high = [fall_time(h, m, r, p, T) for h in h_high]\n",
    "\n",
    "\n",
    "#============\n",
    "#LINEAR MODEL\n",
    "#============\n",
    "lin_model = LinearRegression()\n",
    "lin_model.fit(X, y)\n",
    "\n",
    "#Initalise two new df which are copies of original dataframe\n",
    "X_low = pd.DataFrame(columns=feature_names)\n",
    "X_high = pd.DataFrame(columns=feature_names)\n",
    "\n",
    "#Copies over sample data to low and high dataframes for same length as h_low and h_high\n",
    "for feature in feature_names:\n",
    "    if feature != 'height':\n",
    "        X_low[feature] = [sample[feature]] * len(h_low)\n",
    "        X_high[feature] = [sample[feature]] * len(h_high)\n",
    "\n",
    "#Copy over our height values above and below dataset\n",
    "X_low['height'] = h_low\n",
    "X_high['height'] = h_high\n",
    "\n",
    "#Linear predictions of fall times based on above/ below heights\n",
    "linear_t_low = lin_model.predict(X_low)\n",
    "linear_t_high = lin_model.predict(X_high)\n",
    "\n",
    "\n",
    "#================\n",
    "#MODEL COMPARISON\n",
    "#================\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(h_low, t_low, label='Quadratic Drag Model', color = 'b')\n",
    "plt.plot(h_low, linear_t_low, label='Linear Model', color = 'r')\n",
    "plt.title('Comparison of Quadratic Drag and Linear Models: Heights < 500m')\n",
    "plt.xlabel('Height (m)')\n",
    "plt.ylabel('Fall Time (s)')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(h_high, t_high, label='Quadratic Drag Model', color = 'b')\n",
    "plt.plot(h_high, linear_t_high, label='Linear Model', color = 'r')\n",
    "plt.title('Comparison of Quadratic Drag and Linear Models: Heights > 1000m')\n",
    "plt.xlabel('Height (m)')\n",
    "plt.ylabel('Fall Time (s)')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#Calculate MSE loss for each height subset\n",
    "mse_low = mean_squared_error(t_low, linear_t_low)\n",
    "mse_high = mean_squared_error(t_low, linear_t_high)\n",
    "print(f'MSE loss (h < 500m): {mse_low:.6f}')\n",
    "print(f'MSE loss (h > 1000m): {mse_high:.6f}')\n",
    "\n",
    "#NOTE: Last night the code was working fine (as why I have achieved plots in my report) but I have woken up this morning and the code no longer works - with more time this would've been fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78e6880",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
